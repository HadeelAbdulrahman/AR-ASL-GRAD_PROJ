{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import time\n",
    "from collections import deque\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "mobilenet_model = tf.keras.models.load_model(\"sign_language_model_MobileNetV2.h5\")\n",
    "mlp_model = tf.keras.models.load_model(\"asl_mediapipe_mlp_model.h5\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading the mediapipe keypoints and defining the class labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"asl_mediapipe_keypoints_dataset.csv\")\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(df[\"label\"]) \n",
    "\n",
    "# Correct class labels\n",
    "class_labels = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J',\n",
    "                'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T',\n",
    "                'U', 'V', 'W', 'X', 'Y', 'Z', 'del', 'nothing', 'space']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mediapipe for hand keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "hands = mp_hands.Hands(min_detection_confidence=0.7, min_tracking_confidence=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the height and width of the box, where mobilenet is to be predicting classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "HEIGHT_EXPAND = 220\n",
    "WIDTH_EXPAND = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the predicted sentence\n",
    "predicted_sentence = \"\"\n",
    "last_predicted_label = None\n",
    "last_prediction_time = 0 \n",
    "\n",
    "# 5 seconds cooldown for repeated letters\n",
    "cooldown_time = 5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_landmark_features(hand_landmarks, handedness):\n",
    "    \"\"\"\n",
    "    Extracts and normalizes 21 hand landmarks from MediaPipe.\n",
    "    If the hand is right-handed, mirror it to match left-hand training data.\n",
    "    \"\"\"\n",
    "    landmarks = np.array([[lm.x, lm.y, lm.z] for lm in hand_landmarks.landmark])\n",
    "\n",
    "    # Flip x-coordinates for right hand to match training data\n",
    "    if handedness.classification[0].label == \"Right\":\n",
    "        landmarks[:, 0] = 1 - landmarks[:, 0]\n",
    "\n",
    "    return landmarks.flatten().reshape(1, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main Logic for comparision of confidence score of both models - MobileNet V2 and Mediapipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture('http://192.168.100.33:4747/video')\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Flip frame for a mirrored effect\n",
    "    frame = cv2.flip(frame, 1)\n",
    "\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = hands.process(rgb_frame)\n",
    "\n",
    "    if results.multi_hand_landmarks:\n",
    "        for hand_landmarks, handedness in zip(results.multi_hand_landmarks, results.multi_handedness):\n",
    "            mp_drawing.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "            ### MLP Prediction (Landmark-based)\n",
    "            landmark_features = extract_landmark_features(hand_landmarks, handedness)\n",
    "            mlp_pred = mlp_model.predict(landmark_features)\n",
    "            mlp_class_index = np.argmax(mlp_pred)\n",
    "            mlp_confidence = mlp_pred[0][mlp_class_index]\n",
    "            mlp_label = encoder.inverse_transform([mlp_class_index])[0]\n",
    "\n",
    "            ### Bounding Box Extraction for MobileNetV2\n",
    "            x_min = min([lm.x for lm in hand_landmarks.landmark]) * frame.shape[1]\n",
    "            y_min = min([lm.y for lm in hand_landmarks.landmark]) * frame.shape[0]\n",
    "            x_max = max([lm.x for lm in hand_landmarks.landmark]) * frame.shape[1]\n",
    "            y_max = max([lm.y for lm in hand_landmarks.landmark]) * frame.shape[0]\n",
    "\n",
    "            x_min = max(0, int(x_min - WIDTH_EXPAND))   \n",
    "            y_min = max(0, int(y_min - HEIGHT_EXPAND)) \n",
    "            x_max = min(frame.shape[1], int(x_max + WIDTH_EXPAND))  \n",
    "            y_max = min(frame.shape[0], int(y_max + HEIGHT_EXPAND)) \n",
    "\n",
    "            hand_crop = frame[y_min:y_max, x_min:x_max]\n",
    "\n",
    "            ### MobileNetV2 Prediction (Image-based)\n",
    "            if hand_crop.shape[0] > 0 and hand_crop.shape[1] > 0:\n",
    "                hand_resized = cv2.resize(hand_crop, (128, 128))\n",
    "                hand_resized = np.expand_dims(hand_resized, axis=0) / 255.0  \n",
    "\n",
    "                mobilenet_pred = mobilenet_model.predict(hand_resized)\n",
    "                mobilenet_class_index = np.argmax(mobilenet_pred)\n",
    "                mobilenet_confidence = mobilenet_pred[0][mobilenet_class_index]\n",
    "                mobilenet_label = class_labels[mobilenet_class_index]\n",
    "\n",
    "            ### Decision Fusion: Pick Most Confident Prediction\n",
    "            if mobilenet_confidence > mlp_confidence:\n",
    "                final_label = mobilenet_label\n",
    "                final_confidence = mobilenet_confidence\n",
    "            else:\n",
    "                final_label = mlp_label\n",
    "                final_confidence = mlp_confidence\n",
    "\n",
    "            ### Logic to Prevent Repeated Predictions\n",
    "            current_time = time.time()\n",
    "            if final_label == last_predicted_label:\n",
    "                if current_time - last_prediction_time < cooldown_time:\n",
    "                    final_label = None  # Ignore repeated prediction\n",
    "            else:\n",
    "                last_predicted_label = final_label\n",
    "                last_prediction_time = current_time\n",
    "\n",
    "            ### Sentence Formation Logic\n",
    "            if final_label and final_label not in [\"nothing\", \"del\", \"space\"]:\n",
    "                predicted_sentence += final_label\n",
    "            elif final_label == \"space\":\n",
    "                predicted_sentence += \" \"\n",
    "            elif final_label == \"del\":\n",
    "                predicted_sentence = predicted_sentence[:-1]  # Remove last character\n",
    "\n",
    "            cv2.rectangle(frame, (x_min, y_min), (x_max, y_max), (0, 255, 0), 2)\n",
    "            if final_label:\n",
    "                cv2.putText(frame, f\"{final_label} ({final_confidence:.2f})\", (x_min, y_min - 10),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "    # Create a black bar for displaying sentence\n",
    "    bar_height = 60\n",
    "    frame_height, frame_width, _ = frame.shape\n",
    "    cv2.rectangle(frame, (0, frame_height - bar_height), (frame_width, frame_height), (0, 0, 0), -1)\n",
    "    cv2.putText(frame, predicted_sentence, (50, frame_height - 20),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
    "\n",
    "    cv2.imshow(\"Sign Language Recognition (MediaPipe + MobileNetV2)\", frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the experiment above it can be concluded that models based on convolutional neural networks aren't able to predict the signs correctly and even when combined with predictions of mediapipe multi-level perceptron, it still gives the wrong output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore fine tuning the mediapipe will be the best option available in the market."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine tuning the Mediapipe Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_model = tf.keras.models.load_model(\"asl_mediapipe_mlp_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"‚ñ∏\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"‚ñæ\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LabelEncoder()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LabelEncoder</label><div class=\"sk-toggleable__content\"><pre>LabelEncoder()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"asl_mediapipe_keypoints_dataset.csv\")\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(df[\"label\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "hands = mp_hands.Hands(min_detection_confidence=0.7, min_tracking_confidence=0.7, max_num_hands=2)  # Allow max 2 hands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentence formation logic\n",
    "predicted_sentence = \"\"\n",
    "last_predicted_label = None\n",
    "last_prediction_time = 0\n",
    "\n",
    "# 5 seconds cooldown for repeated letters\n",
    "cooldown_time = 5 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stabilization buffer\n",
    "# Stores last 5 predictions\n",
    "stabilization_window = deque(maxlen=5)\n",
    "\n",
    "# Must match for 4 out of 5 frames\n",
    "stabilization_threshold = 4\n",
    "\n",
    "two_hands_detected = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_landmark_features(hand_landmarks, handedness):\n",
    "    \"\"\"Extract and normalize 21 hand landmarks from MediaPipe.\"\"\"\n",
    "    landmarks = np.array([[lm.x, lm.y, lm.z] for lm in hand_landmarks.landmark])\n",
    "\n",
    "    # Flip x-coordinates for right hand to match left-hand training data\n",
    "    if handedness.classification[0].label == \"Right\":\n",
    "        landmarks[:, 0] = 1 - landmarks[:, 0]\n",
    "\n",
    "    return landmarks.flatten().reshape(1, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is fine tuned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Making of a buffer ensuring that model doesnt predicts images in between the hand sign change\n",
    "- only predic when there is one hand in the frame, when 2 hand appear, give an warning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to connect to DroidCam...\n",
      "‚úÖ Camera initialized. Actual resolution: 1920x1080\n",
      "Starting main loop... Press 'q' to quit.\n",
      "Camera released and windows closed.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[60], line 69\u001b[0m\n\u001b[0;32m     66\u001b[0m frame \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mflip(frame, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     68\u001b[0m rgb_frame \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(frame, cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2RGB)\n\u001b[1;32m---> 69\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mhands\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrgb_frame\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;66;03m# Check if two hands are detected\u001b[39;00m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m results\u001b[38;5;241m.\u001b[39mmulti_hand_landmarks \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(results\u001b[38;5;241m.\u001b[39mmulti_hand_landmarks) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\HADEEL GAMALELDIN\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\mediapipe\\python\\solutions\\hands.py:153\u001b[0m, in \u001b[0;36mHands.process\u001b[1;34m(self, image)\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mprocess\u001b[39m(\u001b[38;5;28mself\u001b[39m, image: np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NamedTuple:\n\u001b[0;32m    133\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Processes an RGB image and returns the hand landmarks and handedness of each detected hand.\u001b[39;00m\n\u001b[0;32m    134\u001b[0m \n\u001b[0;32m    135\u001b[0m \u001b[38;5;124;03m  Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    150\u001b[0m \u001b[38;5;124;03m         right hand) of the detected hand.\u001b[39;00m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 153\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimage\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\HADEEL GAMALELDIN\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\mediapipe\\python\\solution_base.py:325\u001b[0m, in \u001b[0;36mSolutionBase.process\u001b[1;34m(self, input_data)\u001b[0m\n\u001b[0;32m    323\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m data\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m!=\u001b[39m RGB_CHANNELS:\n\u001b[0;32m    324\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInput image must contain three channel rgb data.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 325\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_graph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_packet_to_input_stream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    326\u001b[0m \u001b[43m      \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    327\u001b[0m \u001b[43m      \u001b[49m\u001b[43mpacket\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_packet\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_stream_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    328\u001b[0m \u001b[43m                               \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_simulated_timestamp\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_graph\u001b[38;5;241m.\u001b[39madd_packet_to_input_stream(\n\u001b[0;32m    331\u001b[0m       stream\u001b[38;5;241m=\u001b[39mstream_name,\n\u001b[0;32m    332\u001b[0m       packet\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_packet(input_stream_type,\n\u001b[0;32m    333\u001b[0m                                data)\u001b[38;5;241m.\u001b[39mat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_simulated_timestamp))\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "        bar_height = 60\n",
    "        frame_height, frame_width, _ = frame.shape\n",
    "        cv2.rectangle(frame, (0, frame_height - bar_height), (frame_width, frame_height), (0, 0, 0), -1)\n",
    "\n",
    "        # Display Warning if Two Hands Detected\n",
    "        if two_hands_detected:\n",
    "            cv2.putText(frame, \"Only One Hand Allowed!\", (50, frame_height - 20),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "        else:\n",
    "            cv2.putText(frame, predicted_sentence, (50, frame_height - 20),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
    "\n",
    "        # MINIMIZE DISPLAY - Resize to tiny window (320x240) but keep capture quality high\n",
    "        display_frame = cv2.resize(frame, (320, 240))\n",
    "        cv2.imshow(\"Sign Language Recognition (Fine Tuned MediaPipe)\", display_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "CAMERA IDENTIFICATION TEST\n",
      "============================================================\n",
      "\n",
      "‚úÖ Camera opened at Index: 1\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'cv2' has no attribute 'CAP_PROP_BACKEND_NAME'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[59], line 29\u001b[0m\n\u001b[0;32m     27\u001b[0m height \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(cap\u001b[38;5;241m.\u001b[39mget(cv2\u001b[38;5;241m.\u001b[39mCAP_PROP_FRAME_HEIGHT))\n\u001b[0;32m     28\u001b[0m fps \u001b[38;5;241m=\u001b[39m cap\u001b[38;5;241m.\u001b[39mget(cv2\u001b[38;5;241m.\u001b[39mCAP_PROP_FPS)\n\u001b[1;32m---> 29\u001b[0m backend \u001b[38;5;241m=\u001b[39m cap\u001b[38;5;241m.\u001b[39mget(\u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCAP_PROP_BACKEND_NAME\u001b[49m)\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124müì∑ Resolution: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mwidth\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mx\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mheight\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124müé¨ FPS: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfps\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'cv2' has no attribute 'CAP_PROP_BACKEND_NAME'"
     ]
    }
   ],
   "source": [
    "\n",
    "# TEST: Identify which camera is being used\n",
    "\n",
    "import cv2\n",
    "import time\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"CAMERA IDENTIFICATION TEST\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Try to open the same way as the main code\n",
    "cap = cv2.VideoCapture(1)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    test_index = 0\n",
    "else:\n",
    "    test_index = 1\n",
    "\n",
    "if not cap.isOpened():\n",
    "    cap = cv2.VideoCapture(1, cv2.CAP_DSHOW)\n",
    "    test_index = 1\n",
    "\n",
    "print(f\"\\n‚úÖ Camera opened at Index: {test_index}\")\n",
    "\n",
    "# Get camera properties\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "backend = cap.get(cv2.CAP_PROP_BACKEND_NAME)\n",
    "\n",
    "print(f\"üì∑ Resolution: {width}x{height}\")\n",
    "print(f\"üé¨ FPS: {fps}\")\n",
    "print(f\"üîß Backend: {backend}\")\n",
    "\n",
    "# Try to read a frame\n",
    "ret, frame = cap.read()\n",
    "if ret:\n",
    "    print(f\"\\n‚úÖ Frame captured successfully!\")\n",
    "    print(f\"   Frame shape: {frame.shape}\")\n",
    "    \n",
    "    # Try to identify if it's DroidCam\n",
    "    # DroidCam usually has specific characteristics\n",
    "    if \"droid\" in str(cap.getBackendName()).lower():\n",
    "        print(\"\\nüéØ DETECTED: DroidCam (mobile camera)\")\n",
    "    elif width == 1280 and height == 720:\n",
    "        print(\"\\nüéØ LIKELY: DroidCam (1280x720 is common for DroidCam)\")\n",
    "    elif width == 640 and height == 480:\n",
    "        print(\"\\nü§î UNCLEAR: Could be either PC or DroidCam at 640x480\")\n",
    "    else:\n",
    "        print(f\"\\nüíª LIKELY: PC's built-in camera ({width}x{height})\")\n",
    "else:\n",
    "    print(\"\\n‚ùå Could not read frame from camera\")\n",
    "\n",
    "# Display camera for 3 seconds to visually verify\n",
    "print(\"\\nüì∫ Displaying camera feed for 3 seconds...\")\n",
    "start_time = time.time()\n",
    "while time.time() - start_time < 3:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    # Add text to show which camera it is\n",
    "    cv2.putText(frame, f\"Camera Index: {test_index} | {width}x{height}\", (10, 30),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "    cv2.putText(frame, \"Check if this is your phone (DroidCam) or PC camera\", (10, 60),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 0), 2)\n",
    "    \n",
    "    cv2.imshow(\"Camera Test - Close window to continue\", frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "print(\"\\n‚úÖ Camera test complete!\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Camera found at index 0\n",
      "Camera found at index 1\n",
      "No camera at index 2\n",
      "No camera at index 3\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "# Try indices 0, 1, 2, 3\n",
    "for i in range(4):\n",
    "    cap = cv2.VideoCapture(i)\n",
    "    if cap.isOpened():\n",
    "        print(f\"Camera found at index {i}\")\n",
    "        cap.release()\n",
    "    else:\n",
    "        print(f\"No camera at index {i}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
